# Financial Hallucination Taxonomy



## Definition 

Hallucination in Large Language Models refers to the creation of factually incorrect information in various fields of research and study.\[1] Since finance use cases are usually extremely sensitive and intricate, inaccuracies and misinformation will lead to severe consequences like substantial monetary losses and erosion of trust.\[3]



There are broadly 6 types of hallucinations we see in LLMs in finance-related queries:

## 1. Context Inconsistency

&nbsp;	Refers to instances where a reasoning step is not in line with the contextual information the user/ investor gives the LLM. 

## (2) Logical Inconsistency

&nbsp; We notice this kind of hallucination when there are logical inconsistencies between the current step and the previous step performed by the LLM. 



## (3) Instruction Inconsistency

&nbsp;	This refers to times when there is an inconsistency between the instructions given by the user/investor and the LLM's work.



## (4) Calculation Error

&nbsp;	Refers to instances where a reasoning step makes incorrect calculations, which are easily verifiable by external tools. 



## (5) Factual Inconsistency

&nbsp;	This refers to instances where the LLM presents us with facts which could be grounded as correct information, but in reality present contradictions. 



## (6) Fabrication

&nbsp;	This is when the LLM presents facts which are unverifiable from any document or source and new knowledge in the real world. \[2]

## References

[1] S. M. T. Islam, S. M. M. Zaman, V. Jain, A. Rani, V. Rawte, A. Chadha, and A. Das,  
*“A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models,”*  
survey paper, 2024.

[2] L. Bergeron, I. Buhnila, J. François, and R. State,  
*“HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation,”*  
research paper on hallucination mitigation in RAG systems, 2024.

[3] R. Li, Z. Luo, and X. Du,  
*“FG-PRM: Fine-Grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning,”*  
research paper on mathematical hallucination detection, 2024.




